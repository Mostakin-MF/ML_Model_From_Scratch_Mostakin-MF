{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2fbb813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# Optional imports for convenience (plotting and splitting)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72306af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "    SKLEARN_SPLIT = True\n",
    "except Exception:\n",
    "    SKLEARN_SPLIT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2024cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path, label_column=None, drop_columns=None, header='infer'):\n",
    "    \"\"\"\n",
    "    Load CSV into (X, y, feature_names).\n",
    "    - label_column: name or index of label. If None, assumes last column is label.\n",
    "    - drop_columns: list of columns to drop (names or indices).\n",
    "    Returns:\n",
    "        X: pandas.DataFrame of features\n",
    "        y: pandas.Series of labels\n",
    "        feature_names: list\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=header)\n",
    "\n",
    "    if drop_columns:\n",
    "        df = df.drop(columns=drop_columns)\n",
    "\n",
    "    if label_column is None:\n",
    "        label_column = df.columns[-1]\n",
    "\n",
    "    y = df[label_column].copy()\n",
    "\n",
    "    X = df.drop(columns=[label_column])\n",
    "\n",
    "    return X, y, list(X.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d833482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocess_1(X, fillna='mean', encode_categorical=True, scale=True):\n",
    "    \"\"\"\n",
    "    Preprocess a pandas DataFrame:\n",
    "      - fill missing values (mean/median/zero/ffill/bfill)\n",
    "      - encode categorical columns (one-hot via pd.get_dummies)\n",
    "      - scale numeric features to zero mean + unit std (if scale=True)\n",
    "    Returns: numpy array X_proc, list of column names, scaler dict (mean,std)\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    # fillna\n",
    "    if fillna is not None:\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype.kind in 'biufc':  # numeric\n",
    "                if fillna == 'mean':\n",
    "                    X[col] = X[col].fillna(X[col].mean())\n",
    "                elif fillna == 'median':\n",
    "                    X[col] = X[col].fillna(X[col].median())\n",
    "                elif fillna == 'zero':\n",
    "                    X[col] = X[col].fillna(0)\n",
    "                else:\n",
    "                    X[col] = X[col].fillna(fillna)\n",
    "            else:\n",
    "                X[col] = X[col].fillna(method='ffill').fillna(\n",
    "                    method='bfill').fillna('missing')\n",
    "\n",
    "    # encode categorical\n",
    "    if encode_categorical:\n",
    "        X = pd.get_dummies(X, drop_first=False)\n",
    "\n",
    "    # scaling\n",
    "    scaler = {}\n",
    "    if scale:\n",
    "        means = X.mean()\n",
    "        stds = X.std().replace(0, 1)  # avoid zero-division\n",
    "        X = (X - means) / stds\n",
    "        scaler = {'mean': means, 'std': stds}\n",
    "\n",
    "    return X.values.astype(float), list(X.columns), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d4bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preprocess_2(X, y=None, fillna=\"mean\", fillna_categorical=\"most_frequent\", normalize=True):\n",
    "    \"\"\"\n",
    "    Simple preprocessing function for numeric and categorical features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Input features\n",
    "    y : pd.Series or None\n",
    "        Target (optional, unused here but kept for flexibility)\n",
    "    fillna : str or number, default=\"mean\"\n",
    "        Strategy for numeric columns:\n",
    "        - \"mean\", \"median\", \"zero\", or a custom number\n",
    "    fillna_categorical : str, default=\"most_frequent\"\n",
    "        Strategy for categorical columns:\n",
    "        - \"most_frequent\" (mode)\n",
    "        - \"ffill\" (forward fill → backward fill → 'missing')\n",
    "        - any custom string (e.g., \"unknown\")\n",
    "    normalize : bool, default=True\n",
    "        If True, normalize numeric features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Preprocessed features\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype.kind in 'biufc':  # numeric\n",
    "            if fillna == \"mean\":\n",
    "                X[col] = X[col].fillna(X[col].mean())\n",
    "            elif fillna == \"median\":\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "            elif fillna == \"zero\":\n",
    "                X[col] = X[col].fillna(0)\n",
    "            else:\n",
    "                X[col] = X[col].fillna(fillna)\n",
    "\n",
    "            if normalize:\n",
    "                X[col] = (X[col] - X[col].mean()) / (X[col].std() + 1e-8)\n",
    "\n",
    "        else:  # categorical\n",
    "            if fillna_categorical == \"most_frequent\":\n",
    "                if X[col].mode().empty:   # if column is all NaN\n",
    "                    X[col] = X[col].fillna(\"missing\")\n",
    "                else:\n",
    "                    X[col] = X[col].fillna(X[col].mode()[0])\n",
    "            elif fillna_categorical == \"ffill\":\n",
    "                X[col] = X[col].fillna(method=\"ffill\").fillna(\n",
    "                    method=\"bfill\").fillna(\"missing\")\n",
    "            else:\n",
    "                X[col] = X[col].fillna(fillna_categorical)\n",
    "\n",
    "            X[col] = X[col].astype(str)  # ensure categorical as string\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01da8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Wrapper: uses sklearn if available (preferred), else a small numpy implementation.\n",
    "    Returns numpy arrays: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    if SKLEARN_SPLIT:\n",
    "        X_tr, X_te, y_tr, y_te = sklearn_train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_te, y_tr, y_te\n",
    "    # fallback\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    idx = np.arange(len(X))\n",
    "    rng.shuffle(idx)\n",
    "    split_at = int(len(X) * (1 - test_size))\n",
    "    tr_idx = idx[:split_at]\n",
    "    te_idx = idx[split_at:]\n",
    "    return X[tr_idx], X[te_idx], y[tr_idx], y[te_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c27922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_KNN:\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors implementation (from scratch).\n",
    "    - Supports classification and regression (auto-detected from y).\n",
    "    - distance metrics: 'euclidean' (default) or 'manhattan'\n",
    "    - weights: 'uniform' or 'distance' (distance weighting)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=3, metric='euclidean', weights='uniform'):\n",
    "        self.k = int(k)\n",
    "        if self.k < 1:\n",
    "            raise ValueError(\"k must be >= 1\")\n",
    "        if metric not in ('euclidean', 'manhattan'):\n",
    "            raise ValueError(\"metric must be 'euclidean' or 'manhattan'\")\n",
    "        if weights not in ('uniform', 'distance'):\n",
    "            raise ValueError(\"weights must be 'uniform' or 'distance'\")\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.weights = weights\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Store training data. \n",
    "        X: numpy array (n_samples, n_features). \n",
    "        y: 1D array-like.\"\"\"\n",
    "\n",
    "        self.X_train = np.asarray(X, dtype=float)\n",
    "        self.y_train = np.asarray(y)\n",
    "\n",
    "        if self.X_train.ndim != 2:\n",
    "            raise ValueError(\"X must be 2D array\")\n",
    "        if self.y_train.ndim != 1:\n",
    "            # allow shape (n,1) too\n",
    "            self.y_train = self.y_train.ravel()\n",
    "\n",
    "        self.n_samples = self.X_train.shape[0]\n",
    "        # adjust k if too large\n",
    "        if self.k > self.n_samples:\n",
    "            warnings.warn(\n",
    "                f\"k ({self.k}) is greater than number of training samples ({self.n_samples}); reducing k to {self.n_samples}\")\n",
    "            self.k = self.n_samples\n",
    "\n",
    "        # detect task type\n",
    "        # If y is numeric and not integer-like, treat regression; else classification.\n",
    "        if np.issubdtype(self.y_train.dtype, np.number) and not np.all(np.equal(np.mod(self.y_train, 1), 0)):\n",
    "            self.task = 'regression'\n",
    "        else:\n",
    "            self.task = 'classification'\n",
    "            \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def _pairwise_distances(self, X):\n",
    "        \"\"\"\n",
    "        Compute pairwise distances between each row in X and each row in self.X_train.\n",
    "        Returns shape (n_test, n_train).\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        if self.metric == 'euclidean':\n",
    "            # (a-b)^2 sum over features\n",
    "            # Efficient broadcasting\n",
    "            diff = X[:, None, :] - self.X_train[None, :, :]\n",
    "            d = np.sqrt(np.sum(diff**2, axis=2))\n",
    "        else:  # manhattan\n",
    "            diff = np.abs(X[:, None, :] - self.X_train[None, :, :])\n",
    "            d = np.sum(diff, axis=2)\n",
    "        return d\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels (classification) or values (regression).\n",
    "        Returns numpy array.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"KNN not fitted. Call fit(X,y) first.\")\n",
    "        distances = self._pairwise_distances(X)  # shape (n_test, n_train)\n",
    "        # indices of k nearest neighbors for each test row\n",
    "        k_idx = np.argsort(distances, axis=1)[:, :self.k]  # (n_test, k)\n",
    "        k_dist = np.take_along_axis(distances, k_idx, axis=1)\n",
    "\n",
    "        if self.task == 'regression':\n",
    "            # simple mean or distance-weighted mean\n",
    "            if self.weights == 'uniform':\n",
    "                preds = np.mean(self.y_train[k_idx], axis=1)\n",
    "            else:\n",
    "                # weight by 1/d (avoid div by zero)\n",
    "                w = 1.0 / (k_dist + 1e-12)\n",
    "                numer = np.sum(w * self.y_train[k_idx], axis=1)\n",
    "                denom = np.sum(w, axis=1)\n",
    "                preds = numer / denom\n",
    "            return preds\n",
    "\n",
    "        # classification\n",
    "        if self.weights == 'uniform':\n",
    "            preds = []\n",
    "            for neigh_idx in k_idx:\n",
    "                labels = self.y_train[neigh_idx]\n",
    "                most_common = Counter(labels).most_common()\n",
    "                top_count = most_common[0][1]\n",
    "                # check tie\n",
    "                tied = [lab for lab, cnt in most_common if cnt == top_count]\n",
    "                if len(tied) == 1:\n",
    "                    preds.append(most_common[0][0])\n",
    "                else:\n",
    "                    # tie-breaker: choose the label among tied ones with smallest average distance\n",
    "                    avg_dists = {}\n",
    "                    for t in tied:\n",
    "                        mask = (labels == t)\n",
    "                        avg_dists[t] = np.mean(k_dist[len(preds)][mask])\n",
    "                    chosen = min(avg_dists.items(), key=lambda x: x[1])[0]\n",
    "                    preds.append(chosen)\n",
    "            return np.array(preds)\n",
    "        else:\n",
    "            # distance-weighted voting\n",
    "            preds = []\n",
    "            for i, neigh_idx in enumerate(k_idx):\n",
    "                labels = self.y_train[neigh_idx]\n",
    "                dists = k_dist[i]\n",
    "                weights = 1.0 / (dists + 1e-12)\n",
    "                totals = {}\n",
    "                for lab, w in zip(labels, weights):\n",
    "                    totals[lab] = totals.get(lab, 0.0) + w\n",
    "                # choose label with highest total weight\n",
    "                preds.append(max(totals.items(), key=lambda x: x[1])[0])\n",
    "            return np.array(preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        For classification only: return class probability estimates over neighbors.\n",
    "        Returns list of dicts (or a 2D array with consistent class ordering if desired).\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"KNN not fitted.\")\n",
    "        if self.task != 'classification':\n",
    "            raise RuntimeError(\n",
    "                \"predict_proba only available for classification tasks.\")\n",
    "        distances = self._pairwise_distances(X)\n",
    "        k_idx = np.argsort(distances, axis=1)[:, :self.k]\n",
    "        k_dist = np.take_along_axis(distances, k_idx, axis=1)\n",
    "\n",
    "        proba_list = []\n",
    "        for i, neigh_idx in enumerate(k_idx):\n",
    "            labels = self.y_train[neigh_idx]\n",
    "            if self.weights == 'uniform':\n",
    "                counts = Counter(labels)\n",
    "                total = sum(counts.values())\n",
    "                proba = {lab: counts.get(\n",
    "                    lab, 0) / total for lab in np.unique(self.y_train)}\n",
    "            else:\n",
    "                w = 1.0 / (k_dist[i] + 1e-12)\n",
    "                totals = {}\n",
    "                for lab, wt in zip(labels, w):\n",
    "                    totals[lab] = totals.get(lab, 0.0) + wt\n",
    "                s = sum(totals.values())\n",
    "                proba = {lab: totals.get(lab, 0.0) /\n",
    "                         s for lab in np.unique(self.y_train)}\n",
    "            proba_list.append(proba)\n",
    "        return proba_list\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy for classification, R^2 for regression (simple).\n",
    "        \"\"\"\n",
    "        y = np.asarray(y)\n",
    "        preds = self.predict(X)\n",
    "        if self.task == 'classification':\n",
    "            return np.mean(preds == y)\n",
    "        else:\n",
    "            # compute R^2\n",
    "            ss_res = np.sum((y - preds) ** 2)\n",
    "            ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "            return 1 - ss_res / (ss_tot + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecd9c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 2 0 1 0 0 0 1 2 1 0 2 1 0 1 2 0 2 1 1 1 1 1 2 0 2 1 2 0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:4]  # use petal length & width for 2D visualization\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "clf = Custom_KNN(k=5, metric='euclidean', weights='uniform')\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(\"Predictions:\", preds)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6980253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9362416107382551\n"
     ]
    }
   ],
   "source": [
    "# 1) Load\n",
    "X_df, y_ser, feature_names = load_csv(\"./heart_statlog_cleveland_hungary_final.csv\", label_column=\"target\")\n",
    "\n",
    "# 2) Preprocess (fills missing, one-hot encodes categoricals, scales)\n",
    "X, cols, scaler = simple_preprocess_1(\n",
    "    X_df, fillna='mean', encode_categorical=True, scale=True)\n",
    "\n",
    "# 3) split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y_ser.values, test_size=0.25, random_state=42)\n",
    "\n",
    "# 4) fit & evaluate\n",
    "clf = Custom_KNN(k=9,metric=\"manhattan\", weights='distance')\n",
    "clf.fit(X_tr, y_tr)\n",
    "print(\"Score:\", clf.score(X_te, y_te))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
