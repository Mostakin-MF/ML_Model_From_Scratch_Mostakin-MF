{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ae2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Multi-purpose linear model supporting:\n",
    "      - Regression (MSE) and binary Classification (sigmoid + BCE)\n",
    "      - Multi-feature inputs (X shape: [n_samples, n_features])\n",
    "      - Options: fit_intercept, normalize, batch_size (None => full-batch),\n",
    "                 L2 regularization (penalty='l2', alpha=float)\n",
    "      - Methods: fit(), predict(), predict_proba(), score()\n",
    "    Note: Classification implemented is binary (0/1). For multiclass, use one-vs-rest externally.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=0.01,\n",
    "        n_iters=1000,\n",
    "        task=\"regression\",        # \"regression\" or \"classification\"\n",
    "        batch_size=None,          # None -> full-batch, else integer for mini-batch\n",
    "        fit_intercept=True,\n",
    "        # whether to standardize X (z-score) before training\n",
    "        normalize=False,\n",
    "        penalty=None,             # None or 'l2'\n",
    "        alpha=0.0,                # regularization strength (only for l2)\n",
    "        threshold=0.5,            # classification threshold for predict()\n",
    "    ):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.task = task\n",
    "        self.batch_size = batch_size\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.penalty = penalty\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # model parameters (initialized in fit)\n",
    "        self.weights = None   # shape (n_features,)\n",
    "        self.bias = 0.0\n",
    "        # normalization params (if requested)\n",
    "        self._X_mean = None\n",
    "        self._X_std = None\n",
    "\n",
    "    # ---------------------------\n",
    "    # Utility functions\n",
    "    # ---------------------------\n",
    "    def _prepare_X(self, X, training=False):\n",
    "        \"\"\"\n",
    "        Ensure X is numpy array, optionally normalize using stored params (or compute them if training=True).\n",
    "        \"\"\"\n",
    "        X = np.array(X, dtype=float)\n",
    "        if self.normalize:\n",
    "            if training:\n",
    "                self._X_mean = X.mean(axis=0)\n",
    "                self._X_std = X.std(axis=0)\n",
    "                # avoid division by zero\n",
    "                self._X_std[self._X_std == 0.0] = 1.0\n",
    "            X = (X - self._X_mean) / self._X_std\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        # Numerically stable sigmoid\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training\n",
    "    # ---------------------------\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "          - X: array-like shape (n_samples, n_features)\n",
    "          - y: array-like shape (n_samples,)  for regression: numeric, for classification: {0,1}\n",
    "        \"\"\"\n",
    "        X = self._prepare_X(X, training=True)\n",
    "        y = np.array(y, dtype=float)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # initialize weights and bias\n",
    "        self.weights = np.zeros(n_features, dtype=float)\n",
    "        # bias kept even if fit_intercept False (not used if False)\n",
    "        self.bias = 0.0 if self.fit_intercept else 0.0\n",
    "\n",
    "        # mini-batch logic\n",
    "        if self.batch_size is None:\n",
    "            batch_size = n_samples\n",
    "        else:\n",
    "            batch_size = int(self.batch_size)\n",
    "            batch_size = max(1, min(batch_size, n_samples))\n",
    "\n",
    "        for it in range(self.n_iters):\n",
    "            # shuffle indices for each epoch if using mini-batches\n",
    "            indices = np.arange(n_samples)\n",
    "            if batch_size < n_samples:\n",
    "                np.random.shuffle(indices)\n",
    "\n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                batch_idx = indices[start:start + batch_size]\n",
    "                X_b = X[batch_idx]\n",
    "                y_b = y[batch_idx]\n",
    "                m = X_b.shape[0]  # current batch size\n",
    "\n",
    "                # predictions\n",
    "                linear_output = X_b.dot(\n",
    "                    self.weights) + (self.bias if self.fit_intercept else 0.0)\n",
    "\n",
    "                if self.task == \"regression\":\n",
    "                    # Mean Squared Error gradient:\n",
    "                    # loss = (1/m) * sum((pred - y)^2)\n",
    "                    # dL/dw = (2/m) * X^T (pred - y)\n",
    "                    error = linear_output - y_b\n",
    "                    dw = (2.0 / m) * X_b.T.dot(error)\n",
    "                    db = (2.0 / m) * np.sum(error) if self.fit_intercept else 0.0\n",
    "\n",
    "                    # L2 regularization (ridge)\n",
    "                    if self.penalty == \"l2\" and self.alpha > 0:\n",
    "                        dw += 2.0 * self.alpha * self.weights\n",
    "\n",
    "                elif self.task == \"classification\":\n",
    "                    # Binary cross-entropy:\n",
    "                    # preds = sigmoid(linear_output)\n",
    "                    preds = self._sigmoid(linear_output)\n",
    "                    # gradient of BCE w.r.t. weights: (1/m) * X^T (preds - y)\n",
    "                    error = preds - y_b\n",
    "                    dw = (1.0 / m) * X_b.T.dot(error)\n",
    "                    db = (1.0 / m) * np.sum(error) if self.fit_intercept else 0.0\n",
    "\n",
    "                    if self.penalty == \"l2\" and self.alpha > 0:\n",
    "                        dw += 2.0 * self.alpha * self.weights\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"task must be 'regression' or 'classification'\")\n",
    "\n",
    "                # update parameters\n",
    "                self.weights -= self.lr * dw\n",
    "                if self.fit_intercept:\n",
    "                    self.bias -= self.lr * db\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ---------------------------\n",
    "    # Prediction\n",
    "    # ---------------------------\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        For regression: returns continuous predictions (same as predict)\n",
    "        For classification: returns probability of class 1 (sigmoid output)\n",
    "        \"\"\"\n",
    "        X = self._prepare_X(X, training=False)\n",
    "        linear_output = X.dot(self.weights) + \\\n",
    "            (self.bias if self.fit_intercept else 0.0)\n",
    "        if self.task == \"regression\":\n",
    "            return linear_output\n",
    "        else:\n",
    "            return self._sigmoid(linear_output)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        For regression: continuous outputs\n",
    "        For classification: returns binary labels (0/1) using threshold\n",
    "        \"\"\"\n",
    "        proba_or_pred = self.predict_proba(X)\n",
    "        if self.task == \"regression\":\n",
    "            return proba_or_pred\n",
    "        else:\n",
    "            return (proba_or_pred >= self.threshold).astype(int)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Scoring / metrics\n",
    "    # ---------------------------\n",
    "    def score(self, X, y, metric=None):\n",
    "        \"\"\"\n",
    "        Compute performance score:\n",
    "          - For regression (default): R^2 score (coefficient of determination)\n",
    "              other metric choices: \"mse\", \"mae\", \"r2\"\n",
    "          - For classification (default): accuracy\n",
    "              other metric choices: \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
    "        \"\"\"\n",
    "        X = self._prepare_X(X, training=False)\n",
    "        y = np.array(y)\n",
    "\n",
    "        if self.task == \"regression\":\n",
    "            y_pred = self.predict(X)\n",
    "            if metric is None or metric == \"r2\":\n",
    "                # R^2\n",
    "                ss_res = np.sum((y - y_pred) ** 2)\n",
    "                ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "                # guard against divide by zero\n",
    "                return 1.0 - ss_res / ss_tot if ss_tot != 0 else 0.0\n",
    "            elif metric == \"mse\":\n",
    "                return np.mean((y - y_pred) ** 2)\n",
    "            elif metric == \"mae\":\n",
    "                return np.mean(np.abs(y - y_pred))\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Unsupported metric for regression. Choose 'r2', 'mse' or 'mae'.\")\n",
    "\n",
    "        else:  # classification\n",
    "            y_pred = self.predict(X)\n",
    "            if metric is None or metric == \"accuracy\":\n",
    "                return np.mean(y_pred == y)\n",
    "            elif metric == \"precision\":\n",
    "                tp = np.sum((y_pred == 1) & (y == 1))\n",
    "                fp = np.sum((y_pred == 1) & (y == 0))\n",
    "                return tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            elif metric == \"recall\":\n",
    "                tp = np.sum((y_pred == 1) & (y == 1))\n",
    "                fn = np.sum((y_pred == 0) & (y == 1))\n",
    "                return tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            elif metric == \"f1\":\n",
    "                p = self.score(X, y, metric=\"precision\")\n",
    "                r = self.score(X, y, metric=\"recall\")\n",
    "                return 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Unsupported metric for classification. Choose 'accuracy', 'precision', 'recall' or 'f1'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3976f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49328859060402686\n",
      "Precision: 1.0\n",
      "Recall: 0.10650887573964497\n",
      "F1 Score: 0.0\n",
      "Probabilities: [0.9166172  0.75649564 0.02456942 0.94855636 0.55713288 0.97706413\n",
      " 0.1738604  0.36457849 0.76997118 0.3420673  0.27830342 0.39021214\n",
      " 0.08668475 0.48908156 0.82463153 0.61204875 0.98421599 0.63882514\n",
      " 0.06424871 0.97205305 0.92340993 0.87588039 0.60241156 0.76532619\n",
      " 0.61101988 0.9377167  0.26244947 0.94316716 0.62818966 0.83340232\n",
      " 0.73652749 0.72874281 0.24055403 0.98664635 0.95577443 0.41298055\n",
      " 0.65846499 0.88427005 0.38063548 0.99327885 0.17931648 0.05804783\n",
      " 0.43081355 0.05942756 0.86030348 0.85883804 0.93849992 0.92824058\n",
      " 0.96051827 0.93744201 0.99101366 0.25672271 0.90781112 0.1066918\n",
      " 0.94691775 0.91270345 0.11122039 0.06431108 0.15592645 0.9562983\n",
      " 0.72952329 0.27987921 0.20266946 0.72598946 0.76490021 0.97381475\n",
      " 0.03535259 0.91576644 0.89854197 0.59624142 0.08693864 0.3351853\n",
      " 0.96458301 0.88044535 0.55386561 0.04776253 0.90822313 0.92803923\n",
      " 0.84925964 0.85772294 0.0589426  0.25363264 0.13364411 0.47172353\n",
      " 0.14038478 0.08693833 0.23229347 0.36566305 0.75333466 0.93236076\n",
      " 0.02676275 0.01771419 0.94305729 0.91044485 0.04589412 0.06208593\n",
      " 0.07461832 0.06424871 0.97479357 0.47528118 0.05767914 0.08449965\n",
      " 0.95301419 0.89510219 0.74107429 0.08430882 0.18019523 0.25146272\n",
      " 0.87965054 0.67898389 0.06980805 0.99029543 0.10372925 0.75642139\n",
      " 0.96149512 0.76360191 0.93854346 0.10880639 0.81956848 0.67298782\n",
      " 0.93914278 0.94316716 0.06611203 0.22242871 0.20411121 0.90008338\n",
      " 0.80394432 0.91889786 0.87071906 0.12435059 0.19353995 0.04133364\n",
      " 0.11468599 0.56391042 0.55869149 0.44532193 0.15711814 0.88987613\n",
      " 0.05215304 0.91239147 0.83561906 0.04210003 0.918631   0.18114315\n",
      " 0.82463153 0.85763226 0.80329569 0.93114237 0.91910696 0.48400183\n",
      " 0.05381033 0.01612624 0.09632957 0.10480112 0.29948031 0.3351853\n",
      " 0.8692935  0.95484205 0.48908156 0.08375694 0.06423165 0.02662744\n",
      " 0.09338278 0.09723714 0.83179052 0.23606056 0.03526412 0.16241649\n",
      " 0.12495716 0.9292375  0.60699858 0.85259195 0.965915   0.23064459\n",
      " 0.94512617 0.744079   0.89092063 0.0527802  0.34818714 0.92324846\n",
      " 0.48830866 0.89072465 0.23456161 0.16990346 0.91674125 0.75939741\n",
      " 0.23606056 0.24763448 0.94154659 0.06047853 0.05804783 0.8906282\n",
      " 0.76360191 0.98875557 0.29685193 0.69879023 0.64357888 0.3750062\n",
      " 0.97224888 0.64796164 0.44532193 0.80108526 0.71233271 0.47911929\n",
      " 0.56356675 0.16208908 0.15592645 0.06070421 0.90953909 0.24918892\n",
      " 0.89042661 0.08956876 0.91845749 0.3649798  0.15996846 0.70327798\n",
      " 0.36431363 0.92308863 0.8013801  0.96669432 0.67397254 0.86508608\n",
      " 0.81690941 0.08385093 0.1830936  0.30796096 0.74032746 0.88113438\n",
      " 0.77060318 0.61514003 0.3577344  0.84578561 0.90461816 0.9755196\n",
      " 0.23064459 0.14808219 0.88794625 0.97246423 0.8677463  0.06232537\n",
      " 0.64683368 0.62459028 0.13918156 0.5469562  0.26243893 0.14732955\n",
      " 0.03753081 0.13673973 0.35198506 0.09354922 0.44692266 0.2711787\n",
      " 0.84291742 0.33992696 0.84122624 0.84068164 0.18114315 0.59842895\n",
      " 0.97908186 0.92418619 0.23890992 0.9633152  0.95359739 0.97408786\n",
      " 0.93380671 0.0479679  0.06431108 0.68734004 0.28268048 0.91888026\n",
      " 0.97940447 0.02086267 0.67342171 0.80627531 0.77826875 0.82309226\n",
      " 0.98584696 0.14808219 0.93382784 0.9533132  0.89315896 0.93781757\n",
      " 0.67734917 0.85971544 0.87588039 0.9147124  0.98652405 0.93418948\n",
      " 0.97067114 0.12343345 0.04608389 0.84291742 0.95194058 0.20258904\n",
      " 0.83551148 0.8733796  0.71453256 0.62093329]\n",
      "Predicted classes: [1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1\n",
      " 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0\n",
      " 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1\n",
      " 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1\n",
      " 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
      " 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"./heart_statlog_cleveland_hungary_final.csv\")\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df.drop(columns=[\"target\"])\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train classification model\n",
    "model = LinearRegression(lr=0.01, n_iters=2000, task=\"classification\", normalize=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", model.score(X_test, y_test, metric=\"accuracy\"))\n",
    "print(\"Precision:\", model.score(X_test, y_test, metric=\"precision\"))\n",
    "print(\"Recall:\", model.score(X_test, y_test, metric=\"recall\"))\n",
    "print(\"F1 Score:\", model.score(X_test, y_test, metric=\"f1\"))\n",
    "\n",
    "# Predict probabilities\n",
    "print(\"Probabilities:\", model.predict_proba(X_test))\n",
    "\n",
    "# Predict classes\n",
    "print(\"Predicted classes:\", model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
